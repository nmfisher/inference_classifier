{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "from torch import optim\n",
    "import random \n",
    "from pytorch_transformers.tokenization_distilbert import DistilBertTokenizer\n",
    "from pytorch_transformers.modeling_distilbert import DistilBertModel\n",
    "\n",
    "class Classifier(torch.nn.Module):    \n",
    "    def __init__(self, esz=1536):\n",
    "        super().__init__()\n",
    "        self.dense1 = torch.nn.Linear(\n",
    "            esz, int(esz),\n",
    "        ).cuda()\n",
    "        self.relu1 = torch.nn.ReLU().cuda()\n",
    "        self.dense2 = torch.nn.Linear(\n",
    "            esz, int(esz),\n",
    "        ).cuda()\n",
    "        self.relu2 = torch.nn.ReLU().cuda()\n",
    "        self.dense3 = torch.nn.Linear(\n",
    "            esz, int(esz),\n",
    "        ).cuda()\n",
    "        self.relu3 = torch.nn.ReLU().cuda()\n",
    "        self.out = torch.nn.Linear(\n",
    "            int(esz), 2,\n",
    "        ).cuda()\n",
    "        \n",
    "    def forward(self, input, hidden=None):\n",
    "        return self.out(self.relu3(self.dense3(self.relu2(self.dense2(self.relu1(self.dense1(input.cuda())))))))\n",
    "\n",
    "class Attention(torch.nn.Module):    \n",
    "    def __init__(self, esz=768, seq_len=100):\n",
    "        super().__init__()\n",
    "        self.dense = torch.nn.Linear(\n",
    "            esz, 1\n",
    "        ).cuda()\n",
    "        self.relu = torch.nn.ReLU().cuda()\n",
    "        self.sm = torch.nn.Softmax().cuda()\n",
    "        self.attn = torch.nn.MultiheadAttention(esz, 8).cuda()\n",
    "        \n",
    "    def forward(self, input, hidden=None):\n",
    "        #return self.sm(self.dense(input.cuda()).cuda()).cuda()\n",
    "        return self.attn(input.cuda(), input.cuda(), input.cuda())\n",
    "    \n",
    "class ModelDataset():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train_data = []\n",
    "        self.test_data = []\n",
    "\n",
    "    with open(\"inferences.csv\") as csvDataFile:\n",
    "        csvreader = csv.reader(csvDataFile, delimiter='\\t')\n",
    "        next(csvreader)\n",
    "        for context1, context2, sample1, sample2, inf1, inf2, inf3 in csvreader:\n",
    "            target = self.test_data if random.random() > 0.8 else self.train_data\n",
    "            target.append({\n",
    "                \"context1\":context1,\n",
    "                \"context2\":context2,\n",
    "                \"sample1\":sample1,\n",
    "                \"sample2\":sample2,\n",
    "                \"inf1\":inf1,\n",
    "                \"inf2\":inf2,\n",
    "                \"inf3\":inf3,\n",
    "            })\n",
    "\n",
    "    def sample_train():\n",
    "        positive = random.choice(self.train_data)\n",
    "        while \"inf_random\" not in positive or positive[\"inf1\"] == positive[\"inf_random\"]:\n",
    "            negative = random.choice(self.train_data)\n",
    "            positive[\"inf_random\"] = negative[\"inf1\"]\n",
    "        return positive\n",
    "\n",
    "    def sample_test():\n",
    "        positive = random.choice(self.test_data)\n",
    "        while \"inf_random\" not in positive or positive[\"inf1\"] == positive[\"inf_random\"]:\n",
    "            negative = random.choice(self.train_data)\n",
    "            positive[\"inf_random\"] = negative[\"inf1\"]\n",
    "        return positive\n",
    "    \n",
    "class Model():\n",
    "    def __init__(self, lr=0.00001, esz=768, pad_len=20, path=None):\n",
    "        self.pad_len = pad_len\n",
    "        self.classifier = Classifier(esz=esz*2) \n",
    "        self.context_attention = Attention(esz=esz, seq_len=pad_len)\n",
    "        self.inference_attention = Attention(esz=esz, seq_len=pad_len)\n",
    "        self.utterance_attention = Attention(esz=esz, seq_len=pad_len)\n",
    "        self.optims = {\n",
    "            'classifier': optim.Adam(self.classifier.parameters(), lr=lr),\n",
    "            'context_attention': optim.Adam(self.context_attention.parameters(), lr=lr),\n",
    "            'inference_attention': optim.Adam(self.inference_attention.parameters(), lr=lr),\n",
    "            'utterance_attention': optim.Adam(self.utterance_attention.parameters(), lr=lr),\n",
    "        }\n",
    "        self.classifier_scheduler = torch.optim.lr_scheduler.StepLR(self.optims[\"classifier\"], 0.9)\n",
    "        self.context_attention_scheduler = torch.optim.lr_scheduler.StepLR(self.optims[\"context_attention\"], 0.9)\n",
    "        self.inference_attention_scheduler = torch.optim.lr_scheduler.StepLR(self.optims[\"inference_attention\"], 0.9)\n",
    "        self.utterance_attention_scheduler = torch.optim.lr_scheduler.StepLR(self.optims[\"utterance_attention\"], 0.9)\n",
    "        \n",
    "        if path is not None:\n",
    "            self.classifier.load_state_dict(torch.load(path + \"/classifier.model\"))\n",
    "            self.context_attention.load_state_dict(torch.load(path + \"/context_attention.model\"))\n",
    "            self.inference_attention.load_state_dict(torch.load(path + \"/inference_attention.model\"))\n",
    "            self.utterance_attention.load_state_dict(torch.load(path + \"/utterance_attention.model\"))\n",
    "        \n",
    "        self.loss = 0\n",
    "        self.step = 0\n",
    "        self.c_loss = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for optimizer in self.optims.values():\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "    def update_params(self):\n",
    "        for optimizer in self.optims.values():\n",
    "            optimizer.step()\n",
    "            \n",
    "    def get_and_attend_context(self, embedded):\n",
    "        context_attention_mask1, _ = self.context_attention(embedded[\"context1\"].cuda())\n",
    "        context_attention_mask2, _ = self.context_attention(embedded[\"context2\"].cuda())\n",
    "        context = torch.sum((context_attention_mask1 * embedded[\"context1\"].cuda()) + (context_attention_mask2 * embedded[\"context2\"].cuda()), 1)\n",
    "        return context\n",
    "    \n",
    "    def attend_inference(self, inf):\n",
    "        inference_attention_mask, _ = self.inference_attention(inf.cuda())\n",
    "        return torch.sum(inference_attention_mask * inf.cuda(), 1).cuda()\n",
    "        #return inf.mean(1)\n",
    "\n",
    "    def attend_utterance(self, utterance):\n",
    "        utterance_attention_mask, _ = self.utterance_attention(utterance.cuda())\n",
    "        return torch.sum(utterance_attention_mask * utterance.cuda(), 1).cuda()\n",
    "        #return utterance.mean(1)\n",
    "    \n",
    "    def merge_utterance_with_inference_and_context(self, context, utterance, inference):\n",
    "        #return torch.cat([context,utterance,inference], 1)\n",
    "        return torch.cat([utterance,inference], 1)\n",
    "        \n",
    "    def eval_step(self):\n",
    "        self.classifier.eval()\n",
    "        self.context_attention.eval()\n",
    "        self.inference_attention.eval()\n",
    "        \n",
    "        accuracy = 0\n",
    "        num_steps = 10\n",
    "        for _ in range(num_steps):\n",
    "            xs = sample_test()\n",
    "            #xs = sample_train()\n",
    "            print(\"Sample: %s\" % xs[\"sample1\"])\n",
    "            print(\"Positive Inference 1: %s\" % xs[\"inf1\"])\n",
    "            embedded = self.embed_sample(xs)\n",
    "            \n",
    "            context = self.get_and_attend_context(embedded).cuda()\n",
    "            inf1 = self.attend_inference(embedded[\"inf1\"].cuda()).cuda()\n",
    "            utterance1 = self.attend_utterance(embedded[\"sample1\"].cuda()).cuda()\n",
    "            merged_positive = self.merge_utterance_with_inference_and_context(context, utterance1, inf1).cuda()\n",
    "\n",
    "            pred = self.classifier(merged_positive) .cuda()\n",
    "            print(pred)\n",
    "            pred = torch.argmax(pred, 1)\n",
    "            #if pred.item() == 0:\n",
    "                #print(\"Inference does not follow\")\n",
    "                #accuracy += .append(0)\n",
    "            if pred.item() == 1:\n",
    "                #print(\"Inference follows\")\n",
    "                accuracy += 1\n",
    "\n",
    "            inf_random = self.attend_inference(embedded[\"inf_random\"]).cuda()\n",
    "            merged_negative = self.merge_utterance_with_inference_and_context(context, utterance1, inf_random).cuda()\n",
    "\n",
    "            print(\"Negative Inference 1: %s\" % xs[\"inf_random\"])\n",
    "            pred = self.classifier(merged_negative).cuda() \n",
    "            print(pred)\n",
    "            pred = torch.argmax(pred, 1)\n",
    "            \n",
    "            if pred.item() == 0:\n",
    "            #    print(\"Inference does not follow\")\n",
    "                #accuracy.append(1)\n",
    "                accuracy += 1\n",
    "            #else:\n",
    "                #accuracy.append(0)\n",
    "            #    print(\"Inference follows\")\n",
    "\n",
    "            #context = self.get_and_attend_context(embedded)\n",
    "            #inp = self.attend_inference_and_merge_context(embedded[\"inf1\"], context)\n",
    "        print(\"Accuracy : %f\" % (accuracy / (num_steps * 2)))\n",
    "        print(\"####################\")\n",
    "                \n",
    "    def embed_sample(self, xs):\n",
    "        embedded = {}\n",
    "        for k in [\"context1\",\"context2\",\"sample1\",\"sample2\",\"inf1\",\"inf2\",\"inf3\",\"inf_random\"]:\n",
    "            if k in xs and len(xs[k]) > 0:\n",
    "                tokens = tokenizer.encode(xs[k])\n",
    "                padded = torch.full((1, self.pad_len), tokenizer.pad_token_id, dtype=torch.long)\n",
    "                padded[0,:len(tokens)] = torch.LongTensor([tokens])\n",
    "                embedded[k] = encoder(padded)[0]\n",
    "        return embedded\n",
    "        \n",
    "    def train_step(self):\n",
    "        loss = 0\n",
    "        self.zero_grad()\n",
    "        self.classifier.train()\n",
    "        self.context_attention.train()\n",
    "        self.inference_attention.train()\n",
    "        bsz = 1\n",
    "        batch_xs = []\n",
    "        batch_ys = []\n",
    "        for _ in range(bsz):\n",
    "            xs = sample_train()\n",
    "            #print(xs)\n",
    "            embedded = self.embed_sample(xs)\n",
    "            context = self.get_and_attend_context(embedded).cuda()\n",
    "            inf1 = self.attend_inference(embedded[\"inf1\"]).cuda()\n",
    "            utterance1 = self.attend_utterance(embedded[\"sample1\"]).cuda()\n",
    "            merged_positive = self.merge_utterance_with_inference_and_context(context, utterance1, inf1).cuda()\n",
    "            inf_random = self.attend_inference(embedded[\"inf_random\"]).cuda()\n",
    "            merged_negative = self.merge_utterance_with_inference_and_context(context, utterance1, inf_random).cuda()\n",
    "            batch_xs.append(merged_positive)\n",
    "            batch_xs.append(merged_negative)\n",
    "            batch_ys.append([1])\n",
    "            batch_ys.append([0])\n",
    "        inp = torch.cat(batch_xs, 0).unsqueeze(1).cuda()\n",
    "        \n",
    "            #if \"inf2\" in embedded:\n",
    "            #    inf2 = self.attend_inference_and_merge_context(embedded[\"inf2\"], context)\n",
    "            #    inp = torch.cat([inp, inf2, inf_random], 0)\n",
    "            #    outs += [1,0]\n",
    "            #    if \"inf3\" in embedded:\n",
    "            #        inf3 = self.attend_inference_and_merge_context(embedded[\"inf3\"], context)\n",
    "            #        inp = torch.cat([inp, inf3, inf_random], 0)\n",
    "            #        outs += [1,0]\n",
    "        \n",
    "        outs = torch.LongTensor(batch_ys).cuda()\n",
    "        pred = self.classifier(inp.cuda()).cuda()\n",
    "        loss = self.c_loss(torch.transpose(pred, 1, 2), outs)\n",
    "\n",
    "        self.step += 1\n",
    "        self.loss += loss\n",
    "        if self.step % 50 == 0:\n",
    "            print(self.step)\n",
    "            print(self.loss / 50)\n",
    "            self.loss = 0\n",
    "            self.eval_step()\n",
    "            self.classifier_scheduler.step()\n",
    "            self.context_attention_scheduler.step()\n",
    "            self.inference_attention_scheduler.step()\n",
    "            self.utterance_attention_scheduler.step()\n",
    "            torch.save(self.classifier.state_dict(), \"classifier.model\")\n",
    "            torch.save(self.context_attention.state_dict(), \"context_attention.model\")\n",
    "            torch.save(self.inference_attention.state_dict(), \"inference_attention.model\")\n",
    "            torch.save(self.utterance_attention.state_dict(), \"utterance_attention.model\")\n",
    "\n",
    "        loss.backward()\n",
    "        self.update_params()\n",
    "        \n",
    "        \n",
    "class SequenceModelDataset():\n",
    "    def __init__(self):\n",
    "        self.train_data = []\n",
    "        self.test_data = []\n",
    "\n",
    "        with open(\"sequences.csv\") as csvDataFile:\n",
    "            csvreader = csv.reader(csvDataFile, delimiter='\\t')\n",
    "            next(csvreader)\n",
    "            for utterance, context, response, _, _, _, _ in csvreader:\n",
    "                target = test_data if random.random() > 0.8 else train_data\n",
    "                target.append({\n",
    "                    \"utterance\":utterance,\n",
    "                    \"context\":context,\n",
    "                    \"response\":response,\n",
    "                })\n",
    "\n",
    "    def sample_train():\n",
    "        positive = random.choice(self.train_data)\n",
    "        while \"response_random\" not in positive or positive[\"response\"] == positive[\"response_random\"]:\n",
    "            negative = random.choice(train_data)\n",
    "            positive[\"response_random\"] = negative[\"response\"]\n",
    "        return positive\n",
    "\n",
    "    def sample_test():\n",
    "        positive = random.choice(self.test_data)\n",
    "        while \"response_random\" not in positive or positive[\"response\"] == positive[\"response_random\"]:\n",
    "            negative = random.choice(train_data)\n",
    "            positive[\"response_random\"] = negative[\"response\"]\n",
    "        return positive\n",
    "\n",
    "class SequenceModel(Model):\n",
    "    def __init__(self, lr=0.0001, esz=768, pad_len=30):\n",
    "        super().__init__(lr=lr, esz=esz, pad_len=pad_len)\n",
    "                    \n",
    "    def get_and_attend_context(self, embedded):\n",
    "        context_attention_mask, _ = self.context_attention(embedded[\"context\"])\n",
    "        return context_attention_mask.cuda() * embedded[\"context\"].cuda()\n",
    "    \n",
    "    def embed_sample(self, xs):\n",
    "        embedded = {}\n",
    "        for k in [\"utterance\",\"context\",\"response\",  \"response_random\"]:\n",
    "            if k in xs and len(xs[k]) > 0:\n",
    "                tokens = tokenizer.encode(xs[k])\n",
    "                padded = torch.full((1, self.pad_len), tokenizer.pad_token_id, dtype=torch.long)\n",
    "                padded[0,:len(tokens)] = torch.LongTensor([tokens])\n",
    "                embedded[k] = encoder(padded)[0]\n",
    "        return embedded\n",
    "    \n",
    "    def eval_step(self):\n",
    "        self.classifier.eval()\n",
    "        self.context_attention.eval()\n",
    "        self.inference_attention.eval()\n",
    "        \n",
    "        accuracy = 0\n",
    "        num_steps = 10\n",
    "        for _ in range(num_steps):\n",
    "            xs = sample_test()\n",
    "            print(\"Utterance: %s\" % xs[\"utterance\"])\n",
    "            print(\"Positive response: %s\" % xs[\"response\"])\n",
    "            embedded = self.embed_sample(xs)\n",
    "            \n",
    "            context = SequenceModel.get_and_attend_context(self, embedded)\n",
    "            utterance = self.attend_utterance(embedded[\"utterance\"])\n",
    "            response = self.attend_inference(embedded[\"response\"])\n",
    "            merged_positive = self.merge_utterance_with_inference_and_context(context, utterance, response)\n",
    "\n",
    "            pred = self.classifier(merged_positive) \n",
    "            #print(pred)\n",
    "            pred = torch.argmax(pred, 1)\n",
    "            #if pred.item() == 0:\n",
    "                #print(\"Inference does not follow\")\n",
    "                #accuracy += .append(0)\n",
    "            if pred.item() == 1:\n",
    "                #print(\"Inference follows\")\n",
    "                accuracy += 1\n",
    "\n",
    "            response_random = self.attend_inference(embedded[\"response_random\"])\n",
    "            merged_negative = self.merge_utterance_with_inference_and_context(context, utterance, response_random)\n",
    "\n",
    "            pred = self.classifier(merged_negative) \n",
    "            \n",
    "            pred = torch.argmax(pred, 1)\n",
    "            print(\"Negative Inference 1: %s\" % xs[\"response_random\"])\n",
    "            if pred.item() == 0:\n",
    "            #    print(\"Inference does not follow\")\n",
    "                #accuracy.append(1)\n",
    "                accuracy += 1\n",
    "            #else:\n",
    "                #accuracy.append(0)\n",
    "            #    print(\"Inference follows\")\n",
    "            #print(pred)\n",
    "            #context = self.get_and_attend_context(embedded)\n",
    "            #inp = self.attend_inference_and_merge_context(embedded[\"inf1\"], context)\n",
    "        print(\"Accuracy : %f\" % (accuracy / (num_steps * 2)))\n",
    "                        \n",
    "    def train_step(self):\n",
    "        loss = 0\n",
    "        self.zero_grad()\n",
    "        self.classifier.train()\n",
    "        self.context_attention.train()\n",
    "        self.inference_attention.train()\n",
    "\n",
    "        xs = sample_train()\n",
    "        \n",
    "        embedded = self.embed_sample(xs)\n",
    "        \n",
    "        context = self.get_and_attend_context(embedded)\n",
    "        response = self.attend_inference(embedded[\"response\"])\n",
    "        utterance = self.attend_inference(embedded[\"utterance\"])\n",
    "        merged_positive = self.merge_utterance_with_inference_and_context(context, utterance, response)\n",
    "\n",
    "        response_random = self.attend_inference(embedded[\"response_random\"])\n",
    "        merged_negative = self.merge_utterance_with_inference_and_context(context, utterance, response_random)\n",
    "        inp = torch.cat([merged_positive, merged_negative], 0).unsqueeze(1)\n",
    "        outs = [[1],[0]]\n",
    "        \n",
    "        outs = torch.cuda.LongTensor(outs)\n",
    "\n",
    "        pred = self.classifier(inp)\n",
    "\n",
    "        loss = self.c_loss(torch.transpose(pred, 1, 2), outs)\n",
    "\n",
    "        self.step += 1\n",
    "        self.loss += loss\n",
    "        if self.step % 50 == 0:\n",
    "            print(self.step)\n",
    "            print(self.loss / 50)\n",
    "            self.loss = 0\n",
    "            self.eval_step()\n",
    "            torch.save(self.classifier.state_dict(), \"classifier_seq.model\")\n",
    "            torch.save(self.context_attention.state_dict(), \"context_attention_seq.model\")\n",
    "            torch.save(self.inference_attention.state_dict(), \"inference_attention_seq.model\")\n",
    "            torch.save(self.utterance_attention.state_dict(), \"utterance_attention_seq.model\")\n",
    "            self.classifier_scheduler.step()\n",
    "            self.context_attention_scheduler.step()\n",
    "            self.inference_attention_scheduler.step()\n",
    "            self.utterance_attention_scheduler.step()\n",
    "        loss.backward()\n",
    "        self.update_params()\n",
    "    \n",
    "    def classify(self, utterance, response):\n",
    "    \"\"\"Inference step to determine whether response is coherent to utterance\"\"\"\n",
    "        self.classifier.eval()\n",
    "        self.context_attention.eval()\n",
    "        self.inference_attention.eval()\n",
    "        \n",
    "        embedded = self.embed_sample({\"utterance\":utterance, \"response\":response, \"context\":history[-1]})\n",
    "        \n",
    "        context = self.get_and_attend_context(embedded)\n",
    "        response = self.attend_inference(embedded[\"response\"])\n",
    "        utterance = self.attend_inference(embedded[\"utterance\"])\n",
    "        merged = self.merge_utterance_with_inference_and_context(context, utterance, response)\n",
    "        pred = self.classifier(merged)\n",
    "        return pred[0].item()\n",
    "\n",
    "class RuntimeDataset():\n",
    "    def __init__(self, classifier_model, seq_model):\n",
    "        self.candidates = []\n",
    "        self.history = []\n",
    "        \n",
    "        self.step = 0\n",
    "        self.done = False\n",
    "        self.classifier_model = classifier_model\n",
    "        self.seq_model = seq_model\n",
    "        self.conditions = []\n",
    "        \n",
    "        with open(\"runtime.csv\") as csvDataFile:\n",
    "            csvreader = csv.reader(csvDataFile, delimiter='\\t')\n",
    "            next(csvreader)\n",
    "            for candidate, condition, example1, example2, hint1, hint2 in csvreader:\n",
    "                self.data.append({\n",
    "                    \"candidate\":utterance,\n",
    "                    \"condition\":context,\n",
    "                    \"example1\":response,\n",
    "                    \"example2\":response,\n",
    "                    \"hint1\":response,\n",
    "                    \"hint2\":response,\n",
    "                })\n",
    "    \n",
    "    def next(self):\n",
    "        if self.step == 0:\n",
    "            for index, x in self.data:\n",
    "                if x.condition == \"NULL\":\n",
    "                    self.history.push(x)\n",
    "                    self.data.pop(index)\n",
    "                    return x\n",
    "            raise Exception()\n",
    "        \n",
    "        idx_max = -1\n",
    "        c_max = -1\n",
    "        for index, x in self.data:\n",
    "            classification = self.seq_model.classify(self.history[-1], x.candidate)\n",
    "            if classification > c_max:\n",
    "                c_max = classification\n",
    "                idx_max = index\n",
    "                \n",
    "        return self.data[idx_max].candidate\n",
    "        \n",
    "    def process(self, inp, threshold=0.5):\n",
    "        for index, x in self.data:\n",
    "            score = self.classifier_model.classify(inp, x[\"condition\"])\n",
    "            if score > threshold:\n",
    "                self.conditions.add(x.condition)\n",
    "                seq_classification = self.seq_model.classify(inp, x.candidate)\n",
    "        \n",
    "def main():\n",
    "    # parse command line options\n",
    "    try:\n",
    "        opts, args = getopt.getopt(sys.argv[1:], \"h\", [\"help\"])\n",
    "    except getopt.error, msg:\n",
    "        print msg\n",
    "        print \"for help use --help\"\n",
    "        sys.exit(2)\n",
    "    \n",
    "    opts = {\n",
    "        \"train_classifier\":False,\n",
    "        \"classifier_path\":None,\n",
    "        \"train_seq\":False,\n",
    "        \"seq_path\":None\n",
    "    }\n",
    "    \n",
    "    # process options\n",
    "    for o, a in opts:\n",
    "        if o in (\"-h\", \"--help\"):\n",
    "            print __doc__\n",
    "            sys.exit(0)\n",
    "            continue\n",
    "        if o is \"--train-classifier\":\n",
    "            opts[\"train_classifier\"] = true\n",
    "        if o is \"--train-seq\" :\n",
    "            opts[\"train_seq\"] = true\n",
    "        if o is \"--classifier-path\":\n",
    "            opts[\"classifier_path\"] = a\n",
    "        if o is \"--seq-path\"\":\n",
    "            opts[\"seq_path\"] = a\n",
    "        \n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    special_tokens_dict = {'additional_special_tokens': ['<PLH>', '<s>','</s>']}\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    encoder = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "    encoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    for param in encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    pad_token = tokenizer.pad_token\n",
    "    \n",
    "    model = Model(opts[\"classifier_path\"])\n",
    "    if opts[\"train_classifier\"]:\n",
    "        train_classifier(model)\n",
    "        for i in range(1000):\n",
    "            model.train_step()\n",
    "    \n",
    "    seq_model = SeqModel(opts[\"seq_path\"])\n",
    "    if opts[\"train_seq\"]:\n",
    "        for i in range(1000):\n",
    "            seq_model.train_step()\n",
    "            \n",
    "    runtime_model = RuntimeModel()\n",
    "    \n",
    "    if opts[\"interactive\"]:\n",
    "        while runtime_model.done is False:\n",
    "            print(runtime_model.next())\n",
    "            inp = input(\"> \")\n",
    "            runtime_model.process(inp)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
