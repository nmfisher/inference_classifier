{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "from torch import optim\n",
    "import random \n",
    "import sys\n",
    "from pytorch_transformers.tokenization_distilbert import DistilBertTokenizer\n",
    "from pytorch_transformers.modeling_distilbert import DistilBertModel\n",
    "from scipy.special import softmax\n",
    "import getopt\n",
    "\n",
    "class Classifier(torch.nn.Module):    \n",
    "    def __init__(self, esz=1536):\n",
    "        super().__init__()\n",
    "        self.dense1 = torch.nn.Linear(\n",
    "            esz, int(esz),\n",
    "        ).cuda()\n",
    "        self.relu1 = torch.nn.ReLU().cuda()\n",
    "        self.out = torch.nn.Linear(\n",
    "            int(esz), 2,\n",
    "        ).cuda()\n",
    "        \n",
    "    def forward(self, input, hidden=None):\n",
    "        return self.out(self.relu1(self.dense1(input.cuda())))\n",
    "\n",
    "class Attention(torch.nn.Module):    \n",
    "    def __init__(self, esz=768, seq_len=100):\n",
    "        super().__init__()\n",
    "        self.dense = torch.nn.Linear(\n",
    "            esz, 1\n",
    "        ).cuda()\n",
    "        self.relu = torch.nn.ReLU().cuda()\n",
    "        self.sm = torch.nn.Softmax().cuda()\n",
    "        self.attn = torch.nn.MultiheadAttention(esz, 8).cuda()\n",
    "        \n",
    "    def forward(self, input, hidden=None):\n",
    "        #return self.sm(self.dense(input.cuda()).cuda()).cuda()\n",
    "        return self.attn(input.cuda(), input.cuda(), input.cuda())\n",
    "    \n",
    "class ModelDataset():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.train_data = []\n",
    "        self.test_data = []\n",
    "\n",
    "        with open(\"inferences.csv\") as csvDataFile:\n",
    "            csvreader = csv.reader(csvDataFile, delimiter='\\t')\n",
    "            next(csvreader)\n",
    "            for context1, context2, utterance1, utterance2, inf1, inf2, inf3 in csvreader:\n",
    "                target = self.test_data if random.random() > 0.8 else self.train_data\n",
    "                target.append({\n",
    "                    \"context1\":context1,\n",
    "                    \"context2\":context2,\n",
    "                    \"utterance1\":utterance1,\n",
    "                    \"utterance2\":utterance2,\n",
    "                    \"inf1\":inf1,\n",
    "                    \"inf2\":inf2,\n",
    "                    \"inf3\":inf3,\n",
    "                })\n",
    "\n",
    "    def sample_train(self):\n",
    "        positive = random.choice(self.train_data)\n",
    "        while \"inf_random\" not in positive or positive[\"inf1\"] == positive[\"inf_random\"]:\n",
    "            negative = random.choice(self.train_data)\n",
    "            positive[\"inf_random\"] = negative[\"inf1\"]\n",
    "        return positive\n",
    "\n",
    "    def sample_test(self):\n",
    "        positive = random.choice(self.test_data)\n",
    "        while \"inf_random\" not in positive or positive[\"inf1\"] == positive[\"inf_random\"]:\n",
    "            negative = random.choice(self.train_data)\n",
    "            positive[\"inf_random\"] = negative[\"inf1\"]\n",
    "        return positive\n",
    "    \n",
    "class Model():\n",
    "    def __init__(self, tokenizer=None, encoder=None, lr=0.00001, esz=768, pad_len=20, path=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.encoder = encoder\n",
    "        self.pad_len = pad_len\n",
    "        self.classifier = Classifier(esz=esz*2) \n",
    "        self.context_attention = Attention(esz=esz, seq_len=pad_len)\n",
    "        self.inference_attention = Attention(esz=esz, seq_len=pad_len)\n",
    "        self.utterance_attention = Attention(esz=esz, seq_len=pad_len)\n",
    "        self.optims = {\n",
    "            'classifier': optim.Adam(self.classifier.parameters(), lr=lr),\n",
    "            'context_attention': optim.Adam(self.context_attention.parameters(), lr=lr),\n",
    "            'inference_attention': optim.Adam(self.inference_attention.parameters(), lr=lr),\n",
    "            'utterance_attention': optim.Adam(self.utterance_attention.parameters(), lr=lr),\n",
    "        }\n",
    "        self.classifier_scheduler = torch.optim.lr_scheduler.StepLR(self.optims[\"classifier\"], 0.9)\n",
    "        self.context_attention_scheduler = torch.optim.lr_scheduler.StepLR(self.optims[\"context_attention\"], 0.9)\n",
    "        self.inference_attention_scheduler = torch.optim.lr_scheduler.StepLR(self.optims[\"inference_attention\"], 0.9)\n",
    "        self.utterance_attention_scheduler = torch.optim.lr_scheduler.StepLR(self.optims[\"utterance_attention\"], 0.9)\n",
    "        \n",
    "        if path is not None:\n",
    "            self.classifier.load_state_dict(torch.load(path + \"/classifier.model\"))\n",
    "            self.context_attention.load_state_dict(torch.load(path + \"/context_attention.model\"))\n",
    "            self.inference_attention.load_state_dict(torch.load(path + \"/inference_attention.model\"))\n",
    "            self.utterance_attention.load_state_dict(torch.load(path + \"/utterance_attention.model\"))\n",
    "        \n",
    "        self.loss = 0\n",
    "        self.step = 0\n",
    "        self.c_loss = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for optimizer in self.optims.values():\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "    def update_params(self):\n",
    "        for optimizer in self.optims.values():\n",
    "            optimizer.step()\n",
    "            \n",
    "    def get_and_attend_context(self, embedded):\n",
    "        context_attention_mask1, _ = self.context_attention(embedded[\"context1\"].cuda())\n",
    "        context_attention_mask2, _ = self.context_attention(embedded[\"context2\"].cuda())\n",
    "        context = torch.sum((context_attention_mask1 * embedded[\"context1\"].cuda()) + (context_attention_mask2 * embedded[\"context2\"].cuda()), 1)\n",
    "        return context\n",
    "    \n",
    "    def attend_inference(self, inf):\n",
    "        inference_attention_mask, _ = self.inference_attention(inf.cuda())\n",
    "        return torch.sum(inference_attention_mask * inf.cuda(), 1).cuda()\n",
    "        #return inf.mean(1)\n",
    "\n",
    "    def attend_utterance(self, utterance):\n",
    "        utterance_attention_mask, _ = self.utterance_attention(utterance.cuda())\n",
    "        return torch.sum(utterance_attention_mask * utterance.cuda(), 1).cuda()\n",
    "        #return utterance.mean(1)\n",
    "    \n",
    "    def merge_utterance_with_inference_and_context(self, context, utterance, inference):\n",
    "        #return torch.cat([context,utterance,inference], 1)\n",
    "        return torch.cat([utterance,inference], 1)\n",
    "        \n",
    "    def eval_step(self):\n",
    "        self.classifier.eval()\n",
    "        self.context_attention.eval()\n",
    "        self.inference_attention.eval()\n",
    "        \n",
    "        accuracy = 0\n",
    "        num_steps = 10\n",
    "        for _ in range(num_steps):\n",
    "            xs = sample_test()\n",
    "            #xs = sample_train()\n",
    "            print(\"Sample: %s\" % xs[\"utterance1\"])\n",
    "            print(\"Positive Inference 1: %s\" % xs[\"inf1\"])\n",
    "            embedded = self.embed_sample(xs)\n",
    "            \n",
    "            context = self.get_and_attend_context(embedded).cuda()\n",
    "            inf1 = self.attend_inference(embedded[\"inf1\"].cuda()).cuda()\n",
    "            utterance1 = self.attend_utterance(embedded[\"utterance1\"].cuda()).cuda()\n",
    "            merged_positive = self.merge_utterance_with_inference_and_context(context, utterance1, inf1).cuda()\n",
    "\n",
    "            pred = self.classifier(merged_positive) .cuda()\n",
    "            print(pred)\n",
    "            pred = torch.argmax(pred, 1)\n",
    "            #if pred.item() == 0:\n",
    "                #print(\"Inference does not follow\")\n",
    "                #accuracy += .append(0)\n",
    "            if pred.item() == 1:\n",
    "                #print(\"Inference follows\")\n",
    "                accuracy += 1\n",
    "\n",
    "            inf_random = self.attend_inference(embedded[\"inf_random\"]).cuda()\n",
    "            merged_negative = self.merge_utterance_with_inference_and_context(context, utterance1, inf_random).cuda()\n",
    "\n",
    "            print(\"Negative Inference 1: %s\" % xs[\"inf_random\"])\n",
    "            pred = self.classifier(merged_negative).cuda() \n",
    "            print(pred)\n",
    "            pred = torch.argmax(pred, 1)\n",
    "            \n",
    "            if pred.item() == 0:\n",
    "            #    print(\"Inference does not follow\")\n",
    "                #accuracy.append(1)\n",
    "                accuracy += 1\n",
    "            #else:\n",
    "                #accuracy.append(0)\n",
    "            #    print(\"Inference follows\")\n",
    "\n",
    "            #context = self.get_and_attend_context(embedded)\n",
    "            #inp = self.attend_inference_and_merge_context(embedded[\"inf1\"], context)\n",
    "        print(\"Accuracy : %f\" % (accuracy / (num_steps * 2)))\n",
    "        print(\"####################\")\n",
    "                \n",
    "    def embed_sample(self, xs):\n",
    "        embedded = {}\n",
    "        for k in [\"context1\",\"context2\",\"utterance1\",\"utterance2\",\"inf1\",\"inf2\",\"inf3\",\"inf_random\"]:\n",
    "            if k in xs and len(xs[k]) > 0:\n",
    "                tokens = self.tokenizer.encode(xs[k])\n",
    "                padded = torch.full((1, self.pad_len), self.tokenizer.pad_token_id, dtype=torch.long)\n",
    "                padded[0,:len(tokens)] = torch.LongTensor([tokens])\n",
    "                embedded[k] = self.encoder(padded)[0]\n",
    "        return embedded\n",
    "        \n",
    "    def train_step(self):\n",
    "        loss = 0\n",
    "        self.zero_grad()\n",
    "        self.classifier.train()\n",
    "        self.context_attention.train()\n",
    "        self.inference_attention.train()\n",
    "        bsz = 1\n",
    "        batch_xs = []\n",
    "        batch_ys = []\n",
    "        for _ in range(bsz):\n",
    "            xs = sample_train()\n",
    "            #print(xs)\n",
    "            embedded = self.embed_sample(xs)\n",
    "            context = self.get_and_attend_context(embedded).cuda()\n",
    "            inf1 = self.attend_inference(embedded[\"inf1\"]).cuda()\n",
    "            utterance1 = self.attend_utterance(embedded[\"utterance1\"]).cuda()\n",
    "            merged_positive = self.merge_utterance_with_inference_and_context(context, utterance1, inf1).cuda()\n",
    "            inf_random = self.attend_inference(embedded[\"inf_random\"]).cuda()\n",
    "            merged_negative = self.merge_utterance_with_inference_and_context(context, utterance1, inf_random).cuda()\n",
    "            batch_xs.append(merged_positive)\n",
    "            batch_xs.append(merged_negative)\n",
    "            batch_ys.append([1])\n",
    "            batch_ys.append([0])\n",
    "        inp = torch.cat(batch_xs, 0).unsqueeze(1).cuda()\n",
    "        \n",
    "            #if \"inf2\" in embedded:\n",
    "            #    inf2 = self.attend_inference_and_merge_context(embedded[\"inf2\"], context)\n",
    "            #    inp = torch.cat([inp, inf2, inf_random], 0)\n",
    "            #    outs += [1,0]\n",
    "            #    if \"inf3\" in embedded:\n",
    "            #        inf3 = self.attend_inference_and_merge_context(embedded[\"inf3\"], context)\n",
    "            #        inp = torch.cat([inp, inf3, inf_random], 0)\n",
    "            #        outs += [1,0]\n",
    "        \n",
    "        outs = torch.LongTensor(batch_ys).cuda()\n",
    "        pred = self.classifier(inp.cuda()).cuda()\n",
    "        loss = self.c_loss(torch.transpose(pred, 1, 2), outs)\n",
    "\n",
    "        self.step += 1\n",
    "        self.loss += loss\n",
    "        if self.step % 50 == 0:\n",
    "            print(self.step)\n",
    "            print(self.loss / 50)\n",
    "            self.loss = 0\n",
    "            self.eval_step()\n",
    "            self.classifier_scheduler.step()\n",
    "            self.context_attention_scheduler.step()\n",
    "            self.inference_attention_scheduler.step()\n",
    "            self.utterance_attention_scheduler.step()\n",
    "            torch.save(self.classifier.state_dict(), \"classifier.model\")\n",
    "            torch.save(self.context_attention.state_dict(), \"context_attention.model\")\n",
    "            torch.save(self.inference_attention.state_dict(), \"inference_attention.model\")\n",
    "            torch.save(self.utterance_attention.state_dict(), \"utterance_attention.model\")\n",
    "\n",
    "        loss.backward()\n",
    "        self.update_params()\n",
    "    \n",
    "    def classify(self, utterance, condition, context):\n",
    "        embedded = self.embed_sample({\n",
    "            \"context1\":context,\n",
    "            \"context2\":\"NULL\",\n",
    "            \"utterance1\":utterance,\n",
    "            \"inf1\":condition\n",
    "        })\n",
    "        \n",
    "        context = self.get_and_attend_context(embedded).cuda()\n",
    "        inf1 = self.attend_inference(embedded[\"inf1\"]).cuda()\n",
    "        utterance1 = self.attend_utterance(embedded[\"utterance1\"]).cuda()\n",
    "        inp = self.merge_utterance_with_inference_and_context(context, utterance1, inf1).cuda()\n",
    "        \n",
    "        pred = self.classifier(inp.cuda()).cuda()\n",
    "        return pred\n",
    "        \n",
    "        \n",
    "        \n",
    "class SequenceModelDataset():\n",
    "    def __init__(self):\n",
    "        self.train_data = []\n",
    "        self.test_data = []\n",
    "\n",
    "        with open(\"sequences.csv\") as csvDataFile:\n",
    "            csvreader = csv.reader(csvDataFile, delimiter='\\t')\n",
    "            next(csvreader)\n",
    "            for utterance, context, response, _, _, _, _ in csvreader:\n",
    "                target = test_data if random.random() > 0.8 else train_data\n",
    "                target.append({\n",
    "                    \"utterance\":utterance,\n",
    "                    \"context\":context,\n",
    "                    \"response\":response,\n",
    "                })\n",
    "\n",
    "    def sample_train():\n",
    "        positive = random.choice(self.train_data)\n",
    "        while \"response_random\" not in positive or positive[\"response\"] == positive[\"response_random\"]:\n",
    "            negative = random.choice(train_data)\n",
    "            positive[\"response_random\"] = negative[\"response\"]\n",
    "        return positive\n",
    "\n",
    "    def sample_test():\n",
    "        positive = random.choice(self.test_data)\n",
    "        while \"response_random\" not in positive or positive[\"response\"] == positive[\"response_random\"]:\n",
    "            negative = random.choice(train_data)\n",
    "            positive[\"response_random\"] = negative[\"response\"]\n",
    "        return positive\n",
    "\n",
    "class SequenceModel(Model):\n",
    "    def __init__(self, tokenizer=None, encoder=None, lr=0.0001, esz=768, pad_len=30,path=None):\n",
    "        super().__init__(tokenizer=tokenizer, encoder=encoder, lr=lr, esz=esz, pad_len=pad_len,path=path)\n",
    "                    \n",
    "    def get_and_attend_context(self, embedded):\n",
    "        context_attention_mask, _ = self.context_attention(embedded[\"context\"])\n",
    "        return context_attention_mask.cuda() * embedded[\"context\"].cuda()\n",
    "    \n",
    "    def embed_sample(self, xs):\n",
    "        embedded = {}\n",
    "        for k in [\"utterance\",\"context\",\"response\",  \"response_random\"]:\n",
    "            if k in xs and len(xs[k]) > 0:\n",
    "                tokens = self.tokenizer.encode(xs[k])\n",
    "                padded = torch.full((1, self.pad_len), self.tokenizer.pad_token_id, dtype=torch.long)\n",
    "                padded[0,:len(tokens)] = torch.LongTensor([tokens])\n",
    "                embedded[k] = self.encoder(padded)[0]\n",
    "        return embedded\n",
    "    \n",
    "    def eval_step(self):\n",
    "        self.classifier.eval()\n",
    "        self.context_attention.eval()\n",
    "        self.inference_attention.eval()\n",
    "        \n",
    "        accuracy = 0\n",
    "        num_steps = 10\n",
    "        for _ in range(num_steps):\n",
    "            xs = sample_test()\n",
    "            print(\"Utterance: %s\" % xs[\"utterance\"])\n",
    "            print(\"Positive response: %s\" % xs[\"response\"])\n",
    "            embedded = self.embed_sample(xs)\n",
    "            \n",
    "            context = SequenceModel.get_and_attend_context(self, embedded)\n",
    "            utterance = self.attend_utterance(embedded[\"utterance\"])\n",
    "            response = self.attend_inference(embedded[\"response\"])\n",
    "            merged_positive = self.merge_utterance_with_inference_and_context(context, utterance, response)\n",
    "\n",
    "            pred = self.classifier(merged_positive) \n",
    "            #print(pred)\n",
    "            pred = torch.argmax(pred, 1)\n",
    "            #if pred.item() == 0:\n",
    "                #print(\"Inference does not follow\")\n",
    "                #accuracy += .append(0)\n",
    "            if pred.item() == 1:\n",
    "                #print(\"Inference follows\")\n",
    "                accuracy += 1\n",
    "\n",
    "            response_random = self.attend_inference(embedded[\"response_random\"])\n",
    "            merged_negative = self.merge_utterance_with_inference_and_context(context, utterance, response_random)\n",
    "\n",
    "            pred = self.classifier(merged_negative) \n",
    "            \n",
    "            pred = torch.argmax(pred, 1)\n",
    "            print(\"Negative Inference 1: %s\" % xs[\"response_random\"])\n",
    "            if pred.item() == 0:\n",
    "            #    print(\"Inference does not follow\")\n",
    "                #accuracy.append(1)\n",
    "                accuracy += 1\n",
    "            #else:\n",
    "                #accuracy.append(0)\n",
    "            #    print(\"Inference follows\")\n",
    "            #print(pred)\n",
    "            #context = self.get_and_attend_context(embedded)\n",
    "            #inp = self.attend_inference_and_merge_context(embedded[\"inf1\"], context)\n",
    "        print(\"Accuracy : %f\" % (accuracy / (num_steps * 2)))\n",
    "                        \n",
    "    def train_step(self):\n",
    "        loss = 0\n",
    "        self.zero_grad()\n",
    "        self.classifier.train()\n",
    "        self.context_attention.train()\n",
    "        self.inference_attention.train()\n",
    "\n",
    "        xs = sample_train()\n",
    "        \n",
    "        embedded = self.embed_sample(xs)\n",
    "        \n",
    "        context = self.get_and_attend_context(embedded)\n",
    "        response = self.attend_inference(embedded[\"response\"])\n",
    "        utterance = self.attend_inference(embedded[\"utterance\"])\n",
    "        merged_positive = self.merge_utterance_with_inference_and_context(context, utterance, response)\n",
    "\n",
    "        response_random = self.attend_inference(embedded[\"response_random\"])\n",
    "        merged_negative = self.merge_utterance_with_inference_and_context(context, utterance, response_random)\n",
    "        inp = torch.cat([merged_positive, merged_negative], 0).unsqueeze(1)\n",
    "        outs = [[1],[0]]\n",
    "        \n",
    "        outs = torch.cuda.LongTensor(outs)\n",
    "\n",
    "        pred = self.classifier(inp)\n",
    "\n",
    "        loss = self.c_loss(torch.transpose(pred, 1, 2), outs)\n",
    "\n",
    "        self.step += 1\n",
    "        self.loss += loss\n",
    "        if self.step % 50 == 0:\n",
    "            print(self.step)\n",
    "            print(self.loss / 50)\n",
    "            self.loss = 0\n",
    "            self.eval_step()\n",
    "            torch.save(self.classifier.state_dict(), \"classifier_seq.model\")\n",
    "            torch.save(self.context_attention.state_dict(), \"context_attention_seq.model\")\n",
    "            torch.save(self.inference_attention.state_dict(), \"inference_attention_seq.model\")\n",
    "            torch.save(self.utterance_attention.state_dict(), \"utterance_attention_seq.model\")\n",
    "            self.classifier_scheduler.step()\n",
    "            self.context_attention_scheduler.step()\n",
    "            self.inference_attention_scheduler.step()\n",
    "            self.utterance_attention_scheduler.step()\n",
    "        loss.backward()\n",
    "        self.update_params()\n",
    "    \n",
    "    def classify(self, utterance, context, response):\n",
    "        \"\"\"Inference step to determine whether response is coherent given utterance\"\"\"\n",
    "        self.classifier.eval()\n",
    "        self.context_attention.eval()\n",
    "        self.inference_attention.eval()\n",
    "        \n",
    "        embedded = self.embed_sample({\"utterance\":utterance, \"response\":response, \"context\":context})\n",
    "        \n",
    "        context = self.get_and_attend_context(embedded)\n",
    "        response = self.attend_inference(embedded[\"response\"])\n",
    "        utterance = self.attend_inference(embedded[\"utterance\"])\n",
    "        merged = self.merge_utterance_with_inference_and_context(context, utterance, response)\n",
    "        pred = self.classifier(merged)\n",
    "        return softmax(pred[0].cpu().detach().numpy())[1].item()\n",
    "\n",
    "class RuntimeModel():\n",
    "    def __init__(self, classifier_model, seq_model):\n",
    "        self.candidates = []\n",
    "        self.history = []\n",
    "        \n",
    "        self.step = 0\n",
    "        self.done = False\n",
    "        self.classifier_model = classifier_model\n",
    "        self.seq_model = seq_model\n",
    "        self.conditions = []\n",
    "        \n",
    "        with open(\"runtime.tsv\") as csvDataFile:\n",
    "            csvreader = csv.reader(csvDataFile, delimiter='\\t')\n",
    "            next(csvreader)\n",
    "            for candidate, condition, example1, example2, hint1, hint2 in csvreader:\n",
    "                self.candidates.append({\n",
    "                    \"candidate\":candidate,\n",
    "                    \"condition\":condition,\n",
    "                    \"example1\":example1,\n",
    "                    \"example2\":example2,\n",
    "                    \"hint1\":hint1,\n",
    "                    \"hint2\":hint2,\n",
    "                })\n",
    "    \n",
    "    def next(self):\n",
    "        if self.step == 0:\n",
    "            for index, x in enumerate(self.candidates):\n",
    "                if x[\"condition\"] == \"NULL\":\n",
    "                    self.history.append(x[\"candidate\"])\n",
    "                    self.candidates.pop(index)\n",
    "                    return x[\"candidate\"]\n",
    "            raise Exception()\n",
    "        \n",
    "        idx_max = -1\n",
    "        c_max = -1\n",
    "        for index, x in enumerate(self.candidates):\n",
    "            classification = self.seq_model.classify(x[\"candidate\"], x[\"condition\"], self.history[-1])\n",
    "            if classification > c_max:\n",
    "                c_max = classification\n",
    "                idx_max = index\n",
    "                \n",
    "        return self.candidates[idx_max].candidate\n",
    "        \n",
    "    def process(self, inp, threshold=0.5):\n",
    "        max_coherence_score = -1\n",
    "        max_coherence_index = -1\n",
    "        for index, x in enumerate(self.candidates):\n",
    "#                    def classify(self, utterance, condition, context):\n",
    "\n",
    "            score = self.classifier_model.classify(inp, x[\"condition\"], self.history[-1])\n",
    "            score = softmax(score.cpu().detach().numpy())\n",
    "            print(score)\n",
    "            print(\"Condition Classification Score for %s : %f\" % (x[\"condition\"], score[0][1]))\n",
    "            \n",
    "            if score[0,1] > threshold:\n",
    "                self.conditions.append(x[\"condition\"])\n",
    "                #def classify(self, utterance, context, response):\n",
    "                coherence_score = self.seq_model.classify(inp, self.history[-1], x[\"candidate\"])\n",
    "                print(\"Coherence Score for %s : %f\" % (x[\"candidate\"], coherence_score))\n",
    "                if coherence_score > max_coherence_score:\n",
    "                    max_coherence_score = coherence_score\n",
    "                    max_coherence_index = index\n",
    "        if max_coherence_score == -1:\n",
    "            raise Exception()\n",
    "        return self.candidates[max_coherence_index]\n",
    "        \n",
    "def run_with_opts(opts):\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "    special_tokens_dict = {'additional_special_tokens': ['<PLH>', '<s>','</s>']}\n",
    "    tokenizer.add_special_tokens(special_tokens_dict)\n",
    "    encoder = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "    encoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    for param in encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    pad_token = tokenizer.pad_token\n",
    "    \n",
    "    model = Model(path=opts[\"model_dir\"], tokenizer=tokenizer, encoder=encoder)\n",
    "    if opts[\"train_classifier\"]:\n",
    "        train_classifier(model)\n",
    "        for i in range(1000):\n",
    "            model.train_step()\n",
    "    \n",
    "    seq_model = SequenceModel(path=opts[\"model_dir\"], tokenizer=tokenizer, encoder=encoder)\n",
    "    if opts[\"train_seq\"]:\n",
    "        for i in range(1000):\n",
    "            seq_model.train_step()\n",
    "            \n",
    "    runtime_model = RuntimeModel(model, seq_model)\n",
    "    \n",
    "    if opts[\"interactive\"]:\n",
    "        while runtime_model.done is False:\n",
    "            print(runtime_model.next())\n",
    "            inp = input(\"> \")\n",
    "            runtime_model.process(inp)\n",
    "    return runtime_model\n",
    "    \n",
    "def main():\n",
    "    # parse command line options\n",
    "    try:\n",
    "        opts, args = getopt.getopt(sys.argv[1:], \"h\", [\"help\"])\n",
    "    except Exception as msg:\n",
    "        print(msg)\n",
    "        print(\"for help use --help\")\n",
    "        sys.exit(2)\n",
    "    \n",
    "    opts = {\n",
    "        \"train_classifier\":False,\n",
    "        \"classifier_path\":None,\n",
    "        \"train_seq\":False,\n",
    "        \"seq_path\":None\n",
    "    }\n",
    "    \n",
    "    # process options\n",
    "    for o, a in opts:\n",
    "        if o in (\"-h\", \"--help\"):\n",
    "            print(__doc__)\n",
    "            sys.exit(0)\n",
    "            continue\n",
    "        if o is \"--train-classifier\":\n",
    "            opts[\"train_classifier\"] = true\n",
    "        if o is \"--train-seq\" :\n",
    "            opts[\"train_seq\"] = true\n",
    "        if o is \"--model-dir\":\n",
    "            opts[\"model_dir\"] = a\n",
    "        \n",
    "    run_with_opts(opts)\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Do you play any sports?'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = run_with_opts({\"model_dir\":\"./\", \"train_classifier\":False, \"train_seq\":False, \"interactive\":False})\n",
    "\n",
    "model.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.49739385 0.5026062 ]]\n",
      "Condition Classification Score for You play a sport. : 0.502606\n",
      "Coherence Score for When did you start playing tennis? : 0.528517\n",
      "[[0.4827673 0.5172327]]\n",
      "Condition Classification Score for You play sport : 0.517233\n",
      "Coherence Score for What sport do you play? : 0.534724\n",
      "[[0.48920217 0.5107978 ]]\n",
      "Condition Classification Score for You play a sport that I've never heard of. : 0.510798\n",
      "Coherence Score for Sorry, I've never heard of that sport. : 0.523223\n",
      "[[0.48432046 0.51567954]]\n",
      "Condition Classification Score for You used to play sport : 0.515680\n",
      "Coherence Score for When did you stop playing sport? : 0.533450\n",
      "[[0.4797591  0.52024084]]\n",
      "Condition Classification Score for You play basketball : 0.520241\n",
      "Coherence Score for What position do you play in basketball? : 0.530056\n",
      "[[0.4930463 0.5069537]]\n",
      "Condition Classification Score for You play a position in basketball. : 0.506954\n",
      "Coherence Score for Is that a defensive or offensive position? : 0.526955\n",
      "[[0.47433096 0.52566904]]\n",
      "Condition Classification Score for You don't play basketball. : 0.525669\n",
      "Coherence Score for I don't play basketball either. I prefer soccer. : 0.533064\n",
      "[[0.49739385 0.5026062 ]]\n",
      "Condition Classification Score for You play a sport. : 0.502606\n",
      "Coherence Score for How often do you play? : 0.533112\n",
      "[[0.4760793 0.5239207]]\n",
      "Condition Classification Score for You don't play tennis. : 0.523921\n",
      "Coherence Score for Let's talk about some other sports - like tennis. : 0.525034\n",
      "[[0.4816294  0.51837057]]\n",
      "Condition Classification Score for You play tennis. : 0.518371\n",
      "Coherence Score for Let's talk a bit more about tennis. : 0.529297\n",
      "[[0.5080316  0.49196833]]\n",
      "Condition Classification Score for NULL : 0.491968\n",
      "[[0.5080316  0.49196833]]\n",
      "Condition Classification Score for NULL : 0.491968\n",
      "[[0.5080316  0.49196833]]\n",
      "Condition Classification Score for NULL : 0.491968\n",
      "[[0.5080316  0.49196833]]\n",
      "Condition Classification Score for NULL : 0.491968\n",
      "[[0.5080316  0.49196833]]\n",
      "Condition Classification Score for NULL : 0.491968\n",
      "[[0.5080316  0.49196833]]\n",
      "Condition Classification Score for NULL : 0.491968\n",
      "[[0.5080316  0.49196833]]\n",
      "Condition Classification Score for NULL : 0.491968\n",
      "[[0.5080316  0.49196833]]\n",
      "Condition Classification Score for NULL : 0.491968\n",
      "[[0.5080316  0.49196833]]\n",
      "Condition Classification Score for NULL : 0.491968\n",
      "[[0.50509906 0.49490097]]\n",
      "Condition Classification Score for You think there are two players in a tennis match. : 0.494901\n",
      "[[0.5016426  0.49835745]]\n",
      "Condition Classification Score for You think there are less than two players in a tennis match. : 0.498357\n",
      "[[0.50316054 0.49683946]]\n",
      "Condition Classification Score for You think there are more than two players in a tennis match. : 0.496839\n",
      "[[0.5080316  0.49196833]]\n",
      "Condition Classification Score for NULL : 0.491968\n",
      "[[0.49649537 0.50350463]]\n",
      "Condition Classification Score for You think something else is hanging in the middle of a tennis court. : 0.503505\n",
      "Coherence Score for No, in the middle of a tennis court there is a net. : 0.520312\n",
      "[[0.5056328  0.49436724]]\n",
      "Condition Classification Score for You think there is a net in the middle of a tennis court. : 0.494367\n",
      "[[0.5080316  0.49196833]]\n",
      "Condition Classification Score for NULL : 0.491968\n",
      "[[0.49596408 0.50403595]]\n",
      "Condition Classification Score for You think a tennis ball is not yellow. : 0.504036\n",
      "Coherence Score for No, a tennis ball is yellow. : 0.520290\n",
      "[[0.49615806 0.5038419 ]]\n",
      "Condition Classification Score for You think a tennis ball is yellow. : 0.503842\n",
      "Coherence Score for Correct! Tennis balls are yellow. : 0.525709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'candidate': 'What sport do you play?',\n",
       " 'condition': 'You play sport',\n",
       " 'example1': 'I play sport occasionally',\n",
       " 'example2': 'NULL',\n",
       " 'hint1': 'NULL',\n",
       " 'hint2': 'NULL'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.process(\"I play tennis\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
